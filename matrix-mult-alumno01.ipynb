{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454c61ba-825d-4290-b7ed-135a775fe002",
   "metadata": {},
   "source": [
    "### Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8019c56a-e5ce-469b-a406-9c6056ab9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41 s ± 42.1 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "Result shape: (7000, 7000)\n",
      "Result type: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Large matrices (adjust size as needed)\n",
    "n = 7000  # For very large matrices, ensure you have enough RAM\n",
    "A = np.random.rand(n, n).astype(np.float32)\n",
    "B = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "C = np.dot(A, B)  # warm-up and Matrix multiplication\n",
    "\n",
    "%timeit -r 2 -o np.dot(A, B)\n",
    "\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result type: {C.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e220304-9bec-4755-94da-912417b3cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1080 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/usr/local/lib/python3.13/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1080 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado en PyTorch: torch.Size([7000, 7000])\n",
      "Tiempo en GPU con PyTorch: 0.3725 segundos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# 1. Comprobar si la GPU está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# 2. Convertir arrays de NumPy a Tensores de PyTorch y moverlos a la GPU\n",
    "# Es vital usar .to(device) para que el cálculo no se quede en la CPU\n",
    "A_torch = torch.from_numpy(A).to(device)\n",
    "B_torch = torch.from_numpy(B).to(device)\n",
    "\n",
    "# 3. Función de multiplicación\n",
    "def matmul_pytorch(a, b):\n",
    "    return torch.matmul(a, b)\n",
    "\n",
    "# 4. Medición de tiempo (Sincronizando la GPU)\n",
    "# Al igual que con CuPy, debemos esperar a que la GPU termine para medir bien\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "C_torch = matmul_pytorch(A_torch, B_torch)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Resultado en PyTorch: {C_torch.shape}\")\n",
    "print(f\"Tiempo en GPU con PyTorch: {(end - start):.4f} segundos\")\n",
    "\n",
    "# Opcional: Si necesitas el resultado de vuelta en NumPy/CPU\n",
    "# C_result = C_torch.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad5318-30a5-428e-b6e3-2d53f6808513",
   "metadata": {},
   "source": [
    "Análisis de la actividad extra (PyTorch):\n",
    "\n",
    "La multiplicación de matrices es una operación altamente paralelizable. Mientras que la CPU procesa las filas y columnas de forma secuencial o con hilos limitados, la GPU descompone la matriz 7000×7000 en miles de pequeñas operaciones que se ejecutan simultáneamente en los núcleos CUDA.\n",
    "\n",
    "En mis pruebas, PyTorch en GPU ha demostrado ser significativamente más rápido que NumPy. Además, la integración de PyTorch con el ecosistema de Python es tan fluida que el cambio de código es mínimo, lo que explica su dominio en el campo del Deep Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
